{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jonathan Souty - Inl√§mning 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub: https://github.com/jesab/ml_assignment_1/blob/master/JonathanSouty_ML_Assignment_1_DataPreProcessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "line_break = '\\n' + ('-' * 50) + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the local file path and loading it into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'AmazonDataSales.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get an overview of our data, the pandas function to display all columns when running df.head() is nice here because we are just above the limit where it cuts out a few of our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting option to display all the columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "# using df.head() instead of print(df.head()) gives a better view of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we learn more about each column in the dataset to get a better understanding of our cleaning and transformation needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the dataframe\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "# total rows = 128 975\n",
    "# there are a few duplicates of Order ID, seems like its the same order but with different products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for any odd or unwanted datatypes. For example, here we learn that the 'B2B' column is of the boolean datatype, which we would rather have as integers (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the columns for missing values to help us make decisions on how to handle the inconsistencies. We either fill the missing data using some form of logic or we drop certain columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "print(\"Count of missing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check for duplicates, I did notice that we have a few hidden duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates in all columns\n",
    "print(\"Count of duplicates per column:\")\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I used my own reasoning and with some help of the provided DataProcessMLSemi.py to figure out the cleaning needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 6872 missing values in the Courier Status column out of the ~128k total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Courier Status column aswell as how many null values there are\n",
    "print(df['Courier Status'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to replace the NaN values with 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values with 'Unknown'\n",
    "df['Courier Status'].fillna('Unknown', inplace=True)\n",
    "print(df['Courier Status'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a better idea of how to handle the 'currency' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the currency column\n",
    "print(df['currency'].value_counts(dropna=False))\n",
    "\n",
    "print(line_break)\n",
    "\n",
    "# print the rows where currency is null\n",
    "print(df[df['currency'].isnull()])\n",
    "\n",
    "print(line_break)\n",
    "\n",
    "# check how many rows there are where currency is null but the Amount column isnt null\n",
    "print('Rows with null currency but not null Amount:')\n",
    "print(df[(df['currency'].isnull()) & (df['Amount'].notnull())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all of the missing currency inputs also has ship-country = India, I decided to fill the missing currencies with INR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only have INR currency so its safe to assume that the null values are INR\n",
    "df['currency'].fillna('INR', inplace=True)\n",
    "print(df['currency'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data exploration phase phase I noticed that we have some NaN values in the 'ship-*' columns so here we dive deeper into those columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the ship-city, ship-state, ship-postal-code, ship-country columns for NaN values\n",
    "print(df['ship-city'].isnull().sum())\n",
    "print(df['ship-state'].isnull().sum())\n",
    "print(df['ship-postal-code'].isnull().sum())\n",
    "print(df['ship-country'].isnull().sum())\n",
    "\n",
    "print(line_break)\n",
    "\n",
    "# check the rows where ship-city is not null\n",
    "print(df[df['ship-city'].isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I determined that its safe to fill the NaN values with 'Unknown' because I feel like those values will not have an impact on the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing values with 'Unknown' in columns ship-city, ship-state, ship-country\n",
    "df['ship-city'].fillna('Unknown', inplace=True)\n",
    "df['ship-state'].fillna('Unknown', inplace=True)\n",
    "df['ship-country'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# since ship-postal-code is a float column i will change it to string and change the missing values to 'Unknown', we can leave it as string as we wont be doing any calculations on it\n",
    "df['ship-postal-code'] = df['ship-postal-code'].astype(str)\n",
    "df['ship-postal-code'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Noticed a high amount of hidden duplicates in the 'ship-city' column, as you can see below we have 8 different variates of 'Ahmedabad'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be an issue with how the data is being inputted, will not solve this issue right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the DataFrame for rows where 'ship-city' contains 'Ahmedabad'\n",
    "ahmedabad_df = df[df['ship-city'].str.contains('Ahmedabad', na=False)]\n",
    "\n",
    "# print the value counts of different variations of 'Ahmedabad' in the 'ship-city' column\n",
    "print(ahmedabad_df['ship-city'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two columns has alot of mixed lower/uppercase strings. For now we will just convert all the strings in ship-city and ship-state to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all the strings in 'ship-city' and 'ships-state' columns to lowercase\n",
    "df['ship-city'] = df['ship-city'].str.lower()\n",
    "df['ship-state'] = df['ship-state'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to visualize which columns have a large amount of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all the columns for missing values in percentage compared to total rows and sort them in descending order\n",
    "missing_percentage = df.isnull().sum() / len(df) * 100\n",
    "missing_percentage.sort_values(ascending=False, inplace=True)\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the earlier block 'fulfilled-by', 'Unnamed: 22', 'promotion-ids' columns has 34-69% missing values so we will drop those columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the columns with more than 34-69% values missing\n",
    "df.drop(['fulfilled-by', 'Unnamed: 22', 'promotion-ids'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I cant really justify filling the missing values in the 'Amount' column with anything that will have a positive impact on the model so I decided to drop the ~5% of total rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the Amount column only has about 5% null values i have decided to drop those rows\n",
    "df.dropna(subset=['Amount'], inplace=True)\n",
    "\n",
    "# count total rows of amount\n",
    "print('Total rows remaining:')\n",
    "print(len(df['Amount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the 'Qty' and 'Amount' columns for negative values and correcting them if found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the 'Qty' column has any negative values and correct them if true\n",
    "if (df['Qty'] < 0).any():\n",
    "    print('Qty has negative values. Corrected them now...')\n",
    "    df['Qty'] = df['Qty'].clip(lower=0)\n",
    "else:\n",
    "    print('Qty has no negative values')\n",
    "\n",
    "# check if the 'Amount' column has any negative values and correct them if true\n",
    "if (df['Amount'] < 0).any():\n",
    "    print('Amount has negative values. Corrected them now...')\n",
    "    df['Amount'] = df['Amount'].clip(lower=0)\n",
    "else:\n",
    "    print('Amount has no negative values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I planned to use a normal if islower then convert to upper but apparently that wouldn't work with mixed upper/lower, the lambda function however, is able to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks for any lowercase character within each string and converts them to uppercase\n",
    "lowercase_count = df['Style'].apply(lambda x: any(c.islower() for c in x)).sum()\n",
    "total_row_count = len(df['Style'])\n",
    "\n",
    "if lowercase_count > 0:\n",
    "    print(f'Lowercase characters found in {lowercase_count}/{total_row_count} rows of Style column. Converted them to uppercase.')\n",
    "    df['Style'] = df['Style'].str.upper()\n",
    "else:\n",
    "    print('All characters in Style column are already uppercase.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to test if the code in the block above works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_count = len(df) // 2\n",
    "\n",
    "# convert the first half of the 'Style' column to lowercase\n",
    "df.loc[:half_count, 'Style'] = df.loc[:half_count, 'Style'].str.lower()\n",
    "\n",
    "# same as previous code block just to get the order right\n",
    "if lowercase_count > 0:\n",
    "    print(f'Lowercase characters found in {lowercase_count}/{total_row_count} rows of Style column. Converted them to uppercase.')\n",
    "    df['Style'] = df['Style'].str.upper()\n",
    "else:\n",
    "    print('All characters in Style column are already uppercase.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the 'Date' column to datetime and changing the format to the \"best\" one :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'Date' column to datetime and change format to YYYY-MM-DD\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%m-%d-%y', errors='coerce')\n",
    "\n",
    "# print the first 5 rows of the Date column\n",
    "print(df['Date'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed some mixed lowercase/uppercase in the 'Category' column so I decided to change them all to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change all the 'Category' column values to lowercase\n",
    "df['Category'] = df['Category'].str.lower()\n",
    "\n",
    "# check the 'Category' column\n",
    "print(df['Category'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just doing some midway data exploration to see if there is anything else that needs to be done in the cleaning phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the cleaned dataset\n",
    "print(\"DataFrame Description:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "print(line_break)\n",
    "\n",
    "# check all columns for missing values\n",
    "print(\"Count of missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(line_break)\n",
    "\n",
    "# print total remaining rows\n",
    "print('Total rows remaining:')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we will create new columns using the Date column\n",
    " I chose to only extract Weekday (Monday-Sunday) and Is_weekday TRUE/FALSE (0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a column for 'Weekday' using the 'Date' column\n",
    "df['Weekday'] = df['Date'].dt.day_name()\n",
    "\n",
    "# adding a column for 'Is_weekday' using the 'Date' column\n",
    "# the lambda function will check if the day of the week is 5 or 6 (saturday or sunday) and return 1 if true and 0 if false\n",
    "df['Is_weekday'] = df['Date'].dt.dayofweek.apply(lambda x: 0 if x >= 5 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check if it works we are printing only weekday and is_weekday columns, one where weekday is monday and the other where weekday is saturday\n",
    "print(df[['Weekday', 'Is_weekday']][df['Weekday'] == 'Tuesday'].head())\n",
    "print(line_break)\n",
    "print(df[['Weekday', 'Is_weekday']][df['Weekday'] == 'Saturday'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Converting the 'B2B' column from TRUE/FALSE to binary. I learned that I can instantly convert a bool column to int and all the values TRUE/FALSE will be automatically converted to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the values in the 'B2B' column\n",
    "print(df['B2B'].value_counts(dropna=False))\n",
    "\n",
    "print(line_break)\n",
    "\n",
    "# check the datatype of the 'B2B' column\n",
    "print('Datatype:')\n",
    "print(df['B2B'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the 'B2B' TRUE values to 1 and FALSE values to 0, since the datatype is bool we can just convert it to int\n",
    "df['B2B'] = df['B2B'].astype(int)\n",
    "print(df['B2B'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a better understanding of the 'Amount' column by using the describe function aswell as visualizing it to get a better idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find out more about the 'Amount' column\n",
    "print(\"Description of 'Amount' column:\")\n",
    "print(df['Amount'].describe())\n",
    "\n",
    "print(line_break)\n",
    "\n",
    "# lets visualize the distribution of the 'Amount' column\n",
    "print(\"Distribution of 'Amount' column:\")\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.hist(df['Amount'], bins=100, edgecolor='black')\n",
    "plt.title('Distribution of Amount')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Count')\n",
    "plt.axvline(df['Amount'].max(), color='red', linestyle='dashed', linewidth=1)\n",
    "plt.text(df['Amount'].max() + 500, 10000, 'Max', rotation=360)\n",
    "plt.text(df['Amount'].max() + 500, 9000, df['Amount'].max())\n",
    "plt.axvline(df['Amount'].mean(), color='green', linestyle='dashed', linewidth=1)\n",
    "plt.text(df['Amount'].mean() + 500, 10000, 'Mean', rotation=360)\n",
    "plt.text(df['Amount'].mean() + 500, 9000, round(df['Amount'].mean(), 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the earlier block I learned that we have some extreme outliers, after consulting my colleague mr GPT I decided to do a log transformation on the 'Amount' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding 1 to the 'Amount_Log' column to avoid dividing by 0\n",
    "df['Amount_Log'] = np.log(df['Amount'] + 1)\n",
    "\n",
    "# scaling the 'Amount_Log' column using MinMaxScaler to values between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "df['Amount_Log_Scaled'] = scaler.fit_transform(df[['Amount_Log']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the 'Amount_Log' column\n",
    "print(\"Description of 'Amount_Log' column:\")\n",
    "print(df['Amount_Log'].describe())\n",
    "\n",
    "# visualize the Amount_log column\n",
    "print(\"Distribution of 'Amount_Log' column:\")\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.hist(df['Amount_Log'], bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Amount_Log')\n",
    "plt.xlabel('Amount_Log')\n",
    "plt.ylabel('Count')\n",
    "plt.axvline(df['Amount_Log'].max(), color='red', linestyle='dashed', linewidth=1)\n",
    "plt.text(df['Amount_Log'].max() + 0.1, 10000, 'Max', rotation=360)\n",
    "plt.text(df['Amount_Log'].max() + 0.1, 9000, round(df['Amount_Log'].max(), 2))\n",
    "plt.axvline(df['Amount_Log'].mean(), color='green', linestyle='dashed', linewidth=1)\n",
    "plt.text(df['Amount_Log'].mean() + 0.1, 10000, 'Mean', rotation=360)\n",
    "plt.text(df['Amount_Log'].mean() + 0.1, 9000, round(df['Amount_Log'].mean(), 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the new columns that I generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the 'Amount_Log' column\n",
    "print(\"Description of 'Amount_Log' column:\")\n",
    "print(df['Amount_Log'].describe())\n",
    "\n",
    "print(line_break)\n",
    "\n",
    "print(\"Description of 'Amount_Log_Scaled' column:\")\n",
    "print(df['Amount_Log_Scaled'].describe())\n",
    "\n",
    "print(line_break)\n",
    "\n",
    "# lets visualize the distribution of the 'Amount_Log' column\n",
    "print(\"Distribution of 'Amount_Log_Scaled' column:\")\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.hist(df['Amount_Log_Scaled'], bins=100, edgecolor='black')\n",
    "plt.title('Distribution of Amount_Log_Scaled')\n",
    "plt.xlabel('Amount_Log_Scaled')\n",
    "plt.ylabel('Count')\n",
    "plt.axvline(df['Amount_Log_Scaled'].max(), color='red', linestyle='dashed', linewidth=1)\n",
    "plt.text(df['Amount_Log_Scaled'].max() + 0.1, 10000, 'Max')\n",
    "plt.text(df['Amount_Log_Scaled'].max() + 0.1, 9000, round(df['Amount_Log_Scaled'].max(), 2))\n",
    "plt.axvline(df['Amount_Log_Scaled'].mean(), color='green', linestyle='dashed', linewidth=1)\n",
    "plt.text(df['Amount_Log_Scaled'].mean() + 0.1, 10000, 'Mean')\n",
    "plt.text(df['Amount_Log_Scaled'].mean() + 0.1, 9000, round(df['Amount_Log_Scaled'].mean(), 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing some research I decided to create 5 bins for the 'Amount' column based on the distribution\n",
    "*    Bin 1: 0 to 0.708 (covers up to the 25th percentile)\n",
    "*    Bin 2: 0.708 to 0.742 (covers the 25th to 50th percentile)\n",
    "*    Bin 3: 0.742 to 0.773 (covers the 50th to 75th percentile)\n",
    "*    Bin 4: 0.773 to 0.887 (covers the 75th percentile to an arbitrary upper value)\n",
    "*    Bin 5: 0.887 to 1 (covers the remaining range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = [0, 0.708, 0.742, 0.773, 0.887, 1]\n",
    "\n",
    "bin_labels = ['Min-Q1', 'Q1-Q2', 'Q2-Q3', 'Q3-Q4', 'Q4-Max']\n",
    "\n",
    "df['Amount_Log_Scaled_Bins'] = pd.cut(df['Amount_Log_Scaled'], bins=bin_edges, labels=bin_labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barchart for the 'Amount_Log_Scaled_Bins' column\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "df['Amount_Log_Scaled_Bins'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Amount_Log_Scaled Across Bins')\n",
    "for i in range(df['Amount_Log_Scaled_Bins'].value_counts().shape[0]):\n",
    "    count = df['Amount_Log_Scaled_Bins'].value_counts().values[i]\n",
    "    str_count = str(count)\n",
    "    plt.text(i, count + 1600, str_count, ha='center', va='top')\n",
    "plt.xlabel('Bins')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# describe the 'Amount_Log_Scaled_Bins' column\n",
    "print(\"Description of 'Amount_Log_Scaled_Bins' column:\")\n",
    "print(df['Amount_Log_Scaled_Bins'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describing the 'Qty' column we can see that the mean is around 1 with very low standard deviation aswell as the 4th quantile is 1.00 and max is 8, indicates that we have some extreme outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find out more about the 'Qty' column\n",
    "print(\"Description of 'Qty' column:\")\n",
    "print(df['Qty'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I looked into the data and realized without surprise that the outliers in 'Amount' are on the same rows as the outliers in 'Qty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the columns 'Qty' and 'Amount' to see rows where Qty is larger than 2\n",
    "print(df[['Qty', 'Amount']][df['Qty'] > 2].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decided to do a log transformation on the 'Qty' column aswell, this time I used the log1p function within the numpy library that handles zero values by adding a + 1 before transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will perform a log transformation on the 'Qty', log1p is used to handle zero values to avoid errors\n",
    "df['Qty_Log'] = np.log1p(df['Qty'])\n",
    "\n",
    "# scaling the 'Qty_Log' column using MinMaxScaler to values between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "df['Qty_Log_Scaled'] = scaler.fit_transform(df[['Qty_Log']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also used the MinMaxScaler to get the values between 0 and 1 in the new Qty_Log_Scaled column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Description of 'Qty_Log_Scaled' column:\")\n",
    "print(df['Qty_Log_Scaled'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the columns to see which ones we should do one-hot encoding on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the columns and how many unique values they have\n",
    "print(\"Unique values in each column:\")\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I don't really know which algorithm will be used or how the model will be trained on the data I decided to just encode one column to test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the 'Fulfilment' column into 'Fulfilment_Amazon' and 'Fulfilment_Merchant'\n",
    "df = pd.get_dummies(df, columns=['Fulfilment'], prefix=['Fulfilment'])\n",
    "# and convert the new columns to int\n",
    "df['Fulfilment_Merchant'] = df['Fulfilment_Merchant'].astype(int)\n",
    "df['Fulfilment_Amazon'] = df['Fulfilment_Amazon'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a a lot of factors to consider when choosing which categorical columns to perform OneHot Encoding on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing the finished dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dataframe to xlsx to view the finished dataset (not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save an xlsx file of the cleaned dataset\n",
    "#df.to_excel('AmazonDataSales_cleaned.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_kurs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
